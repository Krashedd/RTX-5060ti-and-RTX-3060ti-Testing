{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bbf8767e",
   "metadata": {},
   "source": [
    "# RESNET-152 TESTING BASE CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64aff1a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in c:\\programming\\indonesia conference paper\\5060ti\\resnet152\\.venv\\lib\\site-packages (25.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Looking in indexes: https://download.pytorch.org/whl/cu129\n",
      "Requirement already satisfied: torch in c:\\programming\\indonesia conference paper\\5060ti\\resnet152\\.venv\\lib\\site-packages (2.8.0+cu129)\n",
      "Requirement already satisfied: torchvision in c:\\programming\\indonesia conference paper\\5060ti\\resnet152\\.venv\\lib\\site-packages (0.23.0+cu129)\n",
      "Requirement already satisfied: torchaudio in c:\\programming\\indonesia conference paper\\5060ti\\resnet152\\.venv\\lib\\site-packages (2.8.0+cu129)\n",
      "Requirement already satisfied: filelock in c:\\programming\\indonesia conference paper\\5060ti\\resnet152\\.venv\\lib\\site-packages (from torch) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\programming\\indonesia conference paper\\5060ti\\resnet152\\.venv\\lib\\site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\programming\\indonesia conference paper\\5060ti\\resnet152\\.venv\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\programming\\indonesia conference paper\\5060ti\\resnet152\\.venv\\lib\\site-packages (from torch) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in c:\\programming\\indonesia conference paper\\5060ti\\resnet152\\.venv\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\programming\\indonesia conference paper\\5060ti\\resnet152\\.venv\\lib\\site-packages (from torch) (2025.12.0)\n",
      "Requirement already satisfied: setuptools in c:\\programming\\indonesia conference paper\\5060ti\\resnet152\\.venv\\lib\\site-packages (from torch) (70.2.0)\n",
      "Requirement already satisfied: numpy in c:\\programming\\indonesia conference paper\\5060ti\\resnet152\\.venv\\lib\\site-packages (from torchvision) (2.3.5)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\programming\\indonesia conference paper\\5060ti\\resnet152\\.venv\\lib\\site-packages (from torchvision) (12.0.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\programming\\indonesia conference paper\\5060ti\\resnet152\\.venv\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\programming\\indonesia conference paper\\5060ti\\resnet152\\.venv\\lib\\site-packages (from jinja2->torch) (2.1.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: onnx in c:\\programming\\indonesia conference paper\\5060ti\\resnet152\\.venv\\lib\\site-packages (1.20.0)\n",
      "Requirement already satisfied: onnxruntime-gpu in c:\\programming\\indonesia conference paper\\5060ti\\resnet152\\.venv\\lib\\site-packages (1.23.2)\n",
      "Requirement already satisfied: numpy>=1.23.2 in c:\\programming\\indonesia conference paper\\5060ti\\resnet152\\.venv\\lib\\site-packages (from onnx) (2.3.5)\n",
      "Requirement already satisfied: protobuf>=4.25.1 in c:\\programming\\indonesia conference paper\\5060ti\\resnet152\\.venv\\lib\\site-packages (from onnx) (6.33.2)\n",
      "Requirement already satisfied: typing_extensions>=4.7.1 in c:\\programming\\indonesia conference paper\\5060ti\\resnet152\\.venv\\lib\\site-packages (from onnx) (4.15.0)\n",
      "Requirement already satisfied: ml_dtypes>=0.5.0 in c:\\programming\\indonesia conference paper\\5060ti\\resnet152\\.venv\\lib\\site-packages (from onnx) (0.5.4)\n",
      "Requirement already satisfied: coloredlogs in c:\\programming\\indonesia conference paper\\5060ti\\resnet152\\.venv\\lib\\site-packages (from onnxruntime-gpu) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in c:\\programming\\indonesia conference paper\\5060ti\\resnet152\\.venv\\lib\\site-packages (from onnxruntime-gpu) (25.9.23)\n",
      "Requirement already satisfied: packaging in c:\\programming\\indonesia conference paper\\5060ti\\resnet152\\.venv\\lib\\site-packages (from onnxruntime-gpu) (25.0)\n",
      "Requirement already satisfied: sympy in c:\\programming\\indonesia conference paper\\5060ti\\resnet152\\.venv\\lib\\site-packages (from onnxruntime-gpu) (1.14.0)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in c:\\programming\\indonesia conference paper\\5060ti\\resnet152\\.venv\\lib\\site-packages (from coloredlogs->onnxruntime-gpu) (10.0)\n",
      "Requirement already satisfied: pyreadline3 in c:\\programming\\indonesia conference paper\\5060ti\\resnet152\\.venv\\lib\\site-packages (from humanfriendly>=9.1->coloredlogs->onnxruntime-gpu) (3.5.4)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\programming\\indonesia conference paper\\5060ti\\resnet152\\.venv\\lib\\site-packages (from sympy->onnxruntime-gpu) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: matplotlib in c:\\programming\\indonesia conference paper\\5060ti\\resnet152\\.venv\\lib\\site-packages (3.10.8)\n",
      "Requirement already satisfied: pillow in c:\\programming\\indonesia conference paper\\5060ti\\resnet152\\.venv\\lib\\site-packages (12.0.0)\n",
      "Requirement already satisfied: tqdm in c:\\programming\\indonesia conference paper\\5060ti\\resnet152\\.venv\\lib\\site-packages (4.67.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\programming\\indonesia conference paper\\5060ti\\resnet152\\.venv\\lib\\site-packages (from matplotlib) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\programming\\indonesia conference paper\\5060ti\\resnet152\\.venv\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\programming\\indonesia conference paper\\5060ti\\resnet152\\.venv\\lib\\site-packages (from matplotlib) (4.61.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\programming\\indonesia conference paper\\5060ti\\resnet152\\.venv\\lib\\site-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: numpy>=1.23 in c:\\programming\\indonesia conference paper\\5060ti\\resnet152\\.venv\\lib\\site-packages (from matplotlib) (2.3.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\programming\\indonesia conference paper\\5060ti\\resnet152\\.venv\\lib\\site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pyparsing>=3 in c:\\programming\\indonesia conference paper\\5060ti\\resnet152\\.venv\\lib\\site-packages (from matplotlib) (3.2.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\programming\\indonesia conference paper\\5060ti\\resnet152\\.venv\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: colorama in c:\\programming\\indonesia conference paper\\5060ti\\resnet152\\.venv\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: six>=1.5 in c:\\programming\\indonesia conference paper\\5060ti\\resnet152\\.venv\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Code Cell 1 Installing torch 12.9\n",
    "%pip install --upgrade pip\n",
    "%pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu129\n",
    "%pip install onnx onnxruntime-gpu\n",
    "%pip install matplotlib pillow tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c867ee09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch VRAM usage capped at ~8 GB (0.50 of total device memory) on cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Code Cell 1.2 â€” Simulated 8 GB VRAM Cap (Allocator-Level)\n",
    "\n",
    "import torch\n",
    "\n",
    "assert torch.cuda.is_available(), \"CUDA is not available.\"\n",
    "\n",
    "# Explicit CUDA device index (required)\n",
    "device_index = 0\n",
    "\n",
    "# RTX 5060 Ti = 16 GB physical VRAM\n",
    "TOTAL_VRAM_GB = 16\n",
    "TARGET_VRAM_GB = 8\n",
    "\n",
    "memory_fraction = TARGET_VRAM_GB / TOTAL_VRAM_GB\n",
    "\n",
    "# Limit how much VRAM PyTorch is allowed to reserve\n",
    "torch.cuda.set_per_process_memory_fraction(memory_fraction, device=device_index)\n",
    "\n",
    "# Clear any cached allocations and reset stats\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "print(\n",
    "    f\"PyTorch VRAM usage capped at ~{TARGET_VRAM_GB} GB \"\n",
    "    f\"({memory_fraction:.2f} of total device memory) on cuda:{device_index}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c992efa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base directory: C:\\Programming\\INDONESIA CONFERENCE PAPER\\5060Ti\\RESNET152\n",
      "Data directory: C:\\Programming\\INDONESIA CONFERENCE PAPER\\5060Ti\\RESNET152\\data\n",
      "Using device: cuda\n",
      "GPU: NVIDIA GeForce RTX 5060 Ti\n",
      "CUDA capability: (12, 0)\n",
      "Torch version: 2.8.0+cu129\n",
      "Torchvision version: 0.23.0+cu129\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Programming\\INDONESIA CONFERENCE PAPER\\5060Ti\\RESNET152\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms as T, models\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "BASE_DIR = Path().resolve()\n",
    "DATA_DIR = BASE_DIR / \"data\"\n",
    "DATA_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"Base directory:\", BASE_DIR)\n",
    "print(\"Data directory:\", DATA_DIR)\n",
    "\n",
    "# Device setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "    print(\"CUDA capability:\", torch.cuda.get_device_capability(0))\n",
    "    print(\"Torch version:\", torch.__version__)\n",
    "    print(\"Torchvision version:\", torchvision.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb98f51",
   "metadata": {},
   "source": [
    "### Download and Extract Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b28ce72c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\programming\\indonesia conference paper\\5060ti\\resnet152\\.venv\\lib\\site-packages (2.32.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\programming\\indonesia conference paper\\5060ti\\resnet152\\.venv\\lib\\site-packages (from requests) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programming\\indonesia conference paper\\5060ti\\resnet152\\.venv\\lib\\site-packages (from requests) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programming\\indonesia conference paper\\5060ti\\resnet152\\.venv\\lib\\site-packages (from requests) (2.6.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programming\\indonesia conference paper\\5060ti\\resnet152\\.venv\\lib\\site-packages (from requests) (2025.11.12)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "images.tar.gz already exists, skipping download.\n",
      "Extracting images.tar.gz ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cedri\\AppData\\Local\\Temp\\ipykernel_2332\\229712417.py:34: DeprecationWarning: Python 3.14 will, by default, filter extracted tar archives and reject files or modify their metadata. Use the filter argument to control this behavior.\n",
      "  tar.extractall(path=extract_to)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n",
      "\n",
      "annotations.tar.gz already exists, skipping download.\n",
      "Extracting annotations.tar.gz ...\n",
      "Done.\n",
      "\n",
      "Dataset download + extraction complete.\n",
      "Images folder: oxford-iiit-pet\\images\n",
      "Annotations folder: oxford-iiit-pet\\annotations\n"
     ]
    }
   ],
   "source": [
    "%pip install requests\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "import requests\n",
    "from tqdm.auto import tqdm\n",
    "import tarfile\n",
    "\n",
    "DATASET_DIR = Path(\"oxford-iiit-pet\")\n",
    "DATASET_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# URLs from the official Oxford-IIIT Pet dataset page\n",
    "URLS = {\n",
    "    \"images\": \"https://www.robots.ox.ac.uk/~vgg/data/pets/data/images.tar.gz\",\n",
    "    \"annotations\": \"https://www.robots.ox.ac.uk/~vgg/data/pets/data/annotations.tar.gz\"\n",
    "}\n",
    "\n",
    "def download_file(url, output_path):\n",
    "    response = requests.get(url, stream=True)\n",
    "    total = int(response.headers.get(\"content-length\", 0))\n",
    "\n",
    "    with open(output_path, \"wb\") as file, tqdm(\n",
    "        desc=f\"Downloading {output_path.name}\",\n",
    "        total=total,\n",
    "        unit=\"B\", unit_scale=True, unit_divisor=1024\n",
    "    ) as bar:\n",
    "        for data in response.iter_content(chunk_size=1024):\n",
    "            size = file.write(data)\n",
    "            bar.update(size)\n",
    "\n",
    "def extract_tar(file_path, extract_to):\n",
    "    print(f\"Extracting {file_path.name} ...\")\n",
    "    with tarfile.open(file_path) as tar:\n",
    "        tar.extractall(path=extract_to)\n",
    "    print(\"Done.\\n\")\n",
    "\n",
    "for name, url in URLS.items():\n",
    "    tar_path = DATASET_DIR / f\"{name}.tar.gz\"\n",
    "\n",
    "    if not tar_path.exists():\n",
    "        download_file(url, tar_path)\n",
    "    else:\n",
    "        print(f\"{tar_path.name} already exists, skipping download.\")\n",
    "\n",
    "    extract_tar(tar_path, DATASET_DIR)\n",
    "\n",
    "print(\"Dataset download + extraction complete.\")\n",
    "print(\"Images folder:\", DATASET_DIR / \"images\")\n",
    "print(\"Annotations folder:\", DATASET_DIR / \"annotations\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd59dd2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 37\n",
      "Train samples: 3680\n",
      "Val samples:   3669\n",
      "Sample tensor shape: torch.Size([3, 420, 420])\n",
      "Sample label: 0 | class name: Abyssinian\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from torchvision.datasets import OxfordIIITPet\n",
    "import torchvision.transforms as T\n",
    "import torchvision.transforms.functional as F\n",
    "import torch\n",
    "\n",
    "DATASET_ROOT = Path(\".\")   # project root\n",
    "BASE_FOLDER = \"oxford-iiit-pet\"\n",
    "DATASET_DIR = DATASET_ROOT / BASE_FOLDER\n",
    "\n",
    "DATASET_DIR = Path(\"dataset_oxford_pet\")\n",
    "\n",
    "class LetterboxToSquare420:\n",
    "    def __init__(self, size=420, fill=0):\n",
    "        self.size = size\n",
    "        self.fill = fill\n",
    "\n",
    "    def __call__(self, img):\n",
    "        # img is a PIL Image (W, H)\n",
    "        w, h = img.size\n",
    "        max_side = max(w, h)\n",
    "        scale = self.size / max_side\n",
    "\n",
    "        new_w = int(round(w * scale))\n",
    "        new_h = int(round(h * scale))\n",
    "\n",
    "        img = F.resize(img, (new_h, new_w))\n",
    "\n",
    "        pad_left = (self.size - new_w) // 2\n",
    "        pad_right = self.size - new_w - pad_left\n",
    "        pad_top = (self.size - new_h) // 2\n",
    "        pad_bottom = self.size - new_h - pad_top\n",
    "\n",
    "        img = F.pad(img, [pad_left, pad_top, pad_right, pad_bottom], fill=self.fill)\n",
    "        return img\n",
    "\n",
    "imagenet_mean = [0.485, 0.456, 0.406]\n",
    "imagenet_std  = [0.229, 0.224, 0.225]\n",
    "\n",
    "common_transform = T.Compose([\n",
    "    LetterboxToSquare420(size=420, fill=0),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=imagenet_mean, std=imagenet_std),\n",
    "])\n",
    "\n",
    "# Datasets: train/val\n",
    "train_dataset = OxfordIIITPet(\n",
    "    root=DATASET_ROOT,\n",
    "    split=\"trainval\",\n",
    "    target_types=\"category\",\n",
    "    transform=common_transform,\n",
    "    download=False,  # already downloaded\n",
    ")\n",
    "\n",
    "val_dataset = OxfordIIITPet(\n",
    "    root=DATASET_ROOT,\n",
    "    split=\"test\",\n",
    "    target_types=\"category\",\n",
    "    transform=common_transform,\n",
    "    download=False,\n",
    ")\n",
    "\n",
    "num_classes = len(train_dataset.classes)\n",
    "print(f\"Number of classes: {num_classes}\")\n",
    "print(f\"Train samples: {len(train_dataset)}\")\n",
    "print(f\"Val samples:   {len(val_dataset)}\")\n",
    "\n",
    "x0, y0 = train_dataset[0]\n",
    "print(\"Sample tensor shape:\", x0.shape)  # expect: [3, 420, 420]\n",
    "print(\"Sample label:\", y0, \"| class name:\", train_dataset.classes[y0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d3de52",
   "metadata": {},
   "source": [
    "### Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f878f4b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: 8\n",
      "Num workers: 0\n",
      "Pin memory: True\n",
      "Train batch shape: torch.Size([8, 3, 420, 420])\n",
      "Train batch labels shape: torch.Size([8])\n",
      "Unique labels in this batch: tensor([ 2,  3,  4,  5, 15, 23, 36])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "BATCH_SIZE = 8\n",
    "NUM_WORKERS = 0      \n",
    "PIN_MEMORY = (device.type == \"cuda\")\n",
    "\n",
    "print(f\"Batch size: {BATCH_SIZE}\")\n",
    "print(f\"Num workers: {NUM_WORKERS}\")\n",
    "print(f\"Pin memory: {PIN_MEMORY}\")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=PIN_MEMORY,\n",
    "    persistent_workers=False,\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=PIN_MEMORY,\n",
    "    persistent_workers=False,\n",
    ")\n",
    "\n",
    "batch_imgs, batch_labels = next(iter(train_loader))\n",
    "print(\"Train batch shape:\", batch_imgs.shape)\n",
    "print(\"Train batch labels shape:\", batch_labels.shape)\n",
    "print(\"Unique labels in this batch:\", batch_labels.unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9c54b1fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes (from dataset): 37\n",
      "Model: ResNet-152\n",
      "Total parameters: 58,219,621\n",
      "Trainable parameters: 58,219,621\n",
      "Device: cuda\n",
      "CUDA device: NVIDIA GeForce RTX 5060 Ti\n",
      "AMP enabled: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cedri\\AppData\\Local\\Temp\\ipykernel_2332\\1394190979.py:37: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=USE_AMP)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision.models import resnet152\n",
    "\n",
    "print(\"Number of classes (from dataset):\", num_classes)\n",
    "\n",
    "model = resnet152(weights=None)\n",
    "\n",
    "in_features = model.fc.in_features\n",
    "model.fc = nn.Linear(in_features, num_classes)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "LEARNING_RATE = 0.01\n",
    "MOMENTUM = 0.9\n",
    "WEIGHT_DECAY = 1e-4\n",
    "EPOCHS = 30\n",
    "\n",
    "optimizer = optim.SGD(\n",
    "    model.parameters(),\n",
    "    lr=LEARNING_RATE,\n",
    "    momentum=MOMENTUM,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    ")\n",
    "\n",
    "scheduler = optim.lr_scheduler.StepLR(\n",
    "    optimizer,\n",
    "    step_size=50,\n",
    "    gamma=0.1,\n",
    ")\n",
    "\n",
    "USE_AMP = (device.type == \"cuda\")\n",
    "\n",
    "from torch.cuda.amp import GradScaler\n",
    "scaler = GradScaler(enabled=USE_AMP)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(\"Model: ResNet-152\")\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "if device.type == \"cuda\":\n",
    "    print(\"CUDA device:\", torch.cuda.get_device_name(0))\n",
    "    print(\"AMP enabled:\", USE_AMP)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "67fbee6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and validation functions defined (AMP-ready).\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "history = {\n",
    "    \"train_loss\": [],\n",
    "    \"val_loss\": [],\n",
    "    \"val_acc\": [],\n",
    "    \"epoch_throughput_imgs_per_sec\": [],\n",
    "}\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer, device, criterion, epoch_idx: int,\n",
    "                    scaler, use_amp: bool):\n",
    "    \"\"\"\n",
    "    Train for one epoch.\n",
    "    Returns:\n",
    "        avg_loss (float)\n",
    "        avg_throughput (float) - images per second across this epoch\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    total_samples = 0\n",
    "\n",
    "    total_batches = 0\n",
    "    throughput_sum = 0.0  # accumulate per-batch throughput\n",
    "\n",
    "    loop = tqdm(loader, desc=f\"Epoch {epoch_idx + 1} - Train\", leave=False)\n",
    "\n",
    "    for inputs, targets in loop:\n",
    "        batch_size = inputs.size(0)\n",
    "        total_samples += batch_size\n",
    "        total_batches += 1\n",
    "\n",
    "        t0 = time.time()\n",
    "\n",
    "        inputs = inputs.to(device, non_blocking=True)\n",
    "        targets = targets.to(device, non_blocking=True)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        if use_amp:\n",
    "            with torch.cuda.amp.autocast():\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        if device.type == \"cuda\":\n",
    "            torch.cuda.synchronize()\n",
    "\n",
    "        t1 = time.time()\n",
    "        batch_time = max(t1 - t0, 1e-6)\n",
    "\n",
    "        throughput = batch_size / batch_time\n",
    "        throughput_sum += throughput\n",
    "\n",
    "        running_loss += loss.item() * batch_size\n",
    "\n",
    "        loop.set_postfix({\n",
    "            \"loss\": loss.item(),\n",
    "            \"thrpt\": f\"{throughput:.1f} img/s\"\n",
    "        })\n",
    "\n",
    "    avg_loss = running_loss / max(total_samples, 1)\n",
    "    avg_throughput = throughput_sum / max(total_batches, 1)\n",
    "\n",
    "    return avg_loss, avg_throughput\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate(model, loader, device, criterion, epoch_idx: int):\n",
    "    \"\"\"\n",
    "    Validation loop.\n",
    "    Returns:\n",
    "        avg_val_loss (float)\n",
    "        val_accuracy (float in [0, 1])\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    total_samples = 0\n",
    "    correct = 0\n",
    "\n",
    "    loop = tqdm(loader, desc=f\"Epoch {epoch_idx + 1} - Val\", leave=False)\n",
    "\n",
    "    for inputs, targets in loop:\n",
    "        batch_size = inputs.size(0)\n",
    "        total_samples += batch_size\n",
    "\n",
    "        inputs = inputs.to(device, non_blocking=True)\n",
    "        targets = targets.to(device, non_blocking=True)\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        running_loss += loss.item() * batch_size\n",
    "\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        correct += (preds == targets).sum().item()\n",
    "\n",
    "        loop.set_postfix({\n",
    "            \"val_loss\": loss.item()\n",
    "        })\n",
    "\n",
    "    avg_val_loss = running_loss / max(total_samples, 1)\n",
    "    val_acc = correct / max(total_samples, 1)\n",
    "\n",
    "    return avg_val_loss, val_acc\n",
    "\n",
    "print(\"Training and validation functions defined (AMP-ready).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1f29b3dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Device: cuda\n",
      "Epochs: 30\n",
      "Batch size: 8\n",
      "AMP enabled: True\n",
      "\n",
      "===== Epoch 1/30 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Train:   0%|          | 0/460 [00:00<?, ?it/s]C:\\Users\\cedri\\AppData\\Local\\Temp\\ipykernel_2332\\201539346.py:41: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "                                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30 summary:\n",
      "  Train loss:       3.9233\n",
      "  Val loss:         5.6734\n",
      "  Val accuracy:     3.19%\n",
      "  Avg throughput:   55.9 images/sec\n",
      "\n",
      "===== Epoch 2/30 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/30 summary:\n",
      "  Train loss:       3.6377\n",
      "  Val loss:         6.0314\n",
      "  Val accuracy:     2.78%\n",
      "  Avg throughput:   55.2 images/sec\n",
      "\n",
      "===== Epoch 3/30 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/30 summary:\n",
      "  Train loss:       3.6225\n",
      "  Val loss:         5.7666\n",
      "  Val accuracy:     3.11%\n",
      "  Avg throughput:   54.8 images/sec\n",
      "\n",
      "===== Epoch 4/30 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/30 summary:\n",
      "  Train loss:       3.5996\n",
      "  Val loss:         12.9039\n",
      "  Val accuracy:     3.22%\n",
      "  Avg throughput:   55.0 images/sec\n",
      "\n",
      "===== Epoch 5/30 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/30 summary:\n",
      "  Train loss:       3.5740\n",
      "  Val loss:         5.8171\n",
      "  Val accuracy:     3.76%\n",
      "  Avg throughput:   55.1 images/sec\n",
      "\n",
      "===== Epoch 6/30 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/30 summary:\n",
      "  Train loss:       3.5453\n",
      "  Val loss:         8.7478\n",
      "  Val accuracy:     4.77%\n",
      "  Avg throughput:   54.4 images/sec\n",
      "\n",
      "===== Epoch 7/30 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/30 summary:\n",
      "  Train loss:       3.5198\n",
      "  Val loss:         4.0953\n",
      "  Val accuracy:     4.72%\n",
      "  Avg throughput:   55.3 images/sec\n",
      "\n",
      "===== Epoch 8/30 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/30 summary:\n",
      "  Train loss:       3.4901\n",
      "  Val loss:         8.4362\n",
      "  Val accuracy:     5.15%\n",
      "  Avg throughput:   55.3 images/sec\n",
      "\n",
      "===== Epoch 9/30 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/30 summary:\n",
      "  Train loss:       3.4628\n",
      "  Val loss:         4.4435\n",
      "  Val accuracy:     5.31%\n",
      "  Avg throughput:   54.8 images/sec\n",
      "\n",
      "===== Epoch 10/30 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/30 summary:\n",
      "  Train loss:       3.4535\n",
      "  Val loss:         4.4518\n",
      "  Val accuracy:     5.29%\n",
      "  Avg throughput:   54.6 images/sec\n",
      "\n",
      "===== Epoch 11/30 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/30 summary:\n",
      "  Train loss:       3.4196\n",
      "  Val loss:         3.9756\n",
      "  Val accuracy:     7.44%\n",
      "  Avg throughput:   53.2 images/sec\n",
      "\n",
      "===== Epoch 12/30 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/30 summary:\n",
      "  Train loss:       3.3925\n",
      "  Val loss:         3.6187\n",
      "  Val accuracy:     4.72%\n",
      "  Avg throughput:   52.9 images/sec\n",
      "\n",
      "===== Epoch 13/30 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/30 summary:\n",
      "  Train loss:       3.3738\n",
      "  Val loss:         3.5959\n",
      "  Val accuracy:     6.98%\n",
      "  Avg throughput:   52.6 images/sec\n",
      "\n",
      "===== Epoch 14/30 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/30 summary:\n",
      "  Train loss:       3.3410\n",
      "  Val loss:         5.4761\n",
      "  Val accuracy:     7.20%\n",
      "  Avg throughput:   52.8 images/sec\n",
      "\n",
      "===== Epoch 15/30 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/30 summary:\n",
      "  Train loss:       3.3407\n",
      "  Val loss:         3.5848\n",
      "  Val accuracy:     7.20%\n",
      "  Avg throughput:   52.6 images/sec\n",
      "\n",
      "===== Epoch 16/30 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/30 summary:\n",
      "  Train loss:       3.3035\n",
      "  Val loss:         3.6815\n",
      "  Val accuracy:     7.74%\n",
      "  Avg throughput:   53.3 images/sec\n",
      "\n",
      "===== Epoch 17/30 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/30 summary:\n",
      "  Train loss:       3.2717\n",
      "  Val loss:         3.7806\n",
      "  Val accuracy:     8.97%\n",
      "  Avg throughput:   52.9 images/sec\n",
      "\n",
      "===== Epoch 18/30 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/30 summary:\n",
      "  Train loss:       3.2349\n",
      "  Val loss:         3.5436\n",
      "  Val accuracy:     8.91%\n",
      "  Avg throughput:   53.3 images/sec\n",
      "\n",
      "===== Epoch 19/30 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/30 summary:\n",
      "  Train loss:       3.1872\n",
      "  Val loss:         3.6878\n",
      "  Val accuracy:     10.38%\n",
      "  Avg throughput:   52.6 images/sec\n",
      "\n",
      "===== Epoch 20/30 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/30 summary:\n",
      "  Train loss:       3.1327\n",
      "  Val loss:         3.1870\n",
      "  Val accuracy:     11.69%\n",
      "  Avg throughput:   52.9 images/sec\n",
      "\n",
      "===== Epoch 21/30 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/30 summary:\n",
      "  Train loss:       3.0720\n",
      "  Val loss:         3.7563\n",
      "  Val accuracy:     11.47%\n",
      "  Avg throughput:   56.8 images/sec\n",
      "\n",
      "===== Epoch 22/30 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/30 summary:\n",
      "  Train loss:       3.0130\n",
      "  Val loss:         3.3162\n",
      "  Val accuracy:     13.90%\n",
      "  Avg throughput:   56.5 images/sec\n",
      "\n",
      "===== Epoch 23/30 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/30 summary:\n",
      "  Train loss:       2.9137\n",
      "  Val loss:         3.7923\n",
      "  Val accuracy:     14.34%\n",
      "  Avg throughput:   56.6 images/sec\n",
      "\n",
      "===== Epoch 24/30 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/30 summary:\n",
      "  Train loss:       2.8509\n",
      "  Val loss:         3.2880\n",
      "  Val accuracy:     14.80%\n",
      "  Avg throughput:   56.7 images/sec\n",
      "\n",
      "===== Epoch 25/30 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/30 summary:\n",
      "  Train loss:       2.7882\n",
      "  Val loss:         3.2213\n",
      "  Val accuracy:     16.49%\n",
      "  Avg throughput:   56.7 images/sec\n",
      "\n",
      "===== Epoch 26/30 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/30 summary:\n",
      "  Train loss:       2.7048\n",
      "  Val loss:         3.0020\n",
      "  Val accuracy:     18.86%\n",
      "  Avg throughput:   56.6 images/sec\n",
      "\n",
      "===== Epoch 27/30 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/30 summary:\n",
      "  Train loss:       2.6517\n",
      "  Val loss:         4.4681\n",
      "  Val accuracy:     15.54%\n",
      "  Avg throughput:   56.7 images/sec\n",
      "\n",
      "===== Epoch 28/30 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/30 summary:\n",
      "  Train loss:       2.5524\n",
      "  Val loss:         2.9512\n",
      "  Val accuracy:     20.71%\n",
      "  Avg throughput:   56.8 images/sec\n",
      "\n",
      "===== Epoch 29/30 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/30 summary:\n",
      "  Train loss:       2.4631\n",
      "  Val loss:         3.6162\n",
      "  Val accuracy:     16.52%\n",
      "  Avg throughput:   55.8 images/sec\n",
      "\n",
      "===== Epoch 30/30 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/30 summary:\n",
      "  Train loss:       2.3545\n",
      "  Val loss:         3.4144\n",
      "  Val accuracy:     19.38%\n",
      "  Avg throughput:   56.7 images/sec\n",
      "\n",
      "===== Training complete =====\n",
      "Total training time: 67.03 minutes\n",
      "Peak VRAM usage:    3162.1 MB (3.088 GB)\n",
      "Best val accuracy:  20.71%\n",
      "Final val accuracy: 19.38%\n",
      "\n",
      "NOTE:\n",
      "- Record avg GPU Util, avg VRAM, and avg power from HWiNFO at mid-training.\n",
      "- Later, we can compute performance per watt = best_epoch_throughput / avg_power_W.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "print(\"Starting training...\")\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"Epochs: {EPOCHS}\")\n",
    "print(f\"Batch size: {BATCH_SIZE}\")\n",
    "print(f\"AMP enabled: {USE_AMP}\")\n",
    "\n",
    "if device.type == \"cuda\":\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "total_train_start = time.time()\n",
    "\n",
    "best_val_acc = 0.0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"\\n===== Epoch {epoch + 1}/{EPOCHS} =====\")\n",
    "\n",
    "    train_loss, avg_throughput = train_one_epoch(\n",
    "        model=model,\n",
    "        loader=train_loader,\n",
    "        optimizer=optimizer,\n",
    "        device=device,\n",
    "        criterion=criterion,\n",
    "        epoch_idx=epoch,\n",
    "        scaler=scaler,\n",
    "        use_amp=USE_AMP,\n",
    "    )\n",
    "\n",
    "    val_loss, val_acc = validate(\n",
    "        model=model,\n",
    "        loader=val_loader,\n",
    "        device=device,\n",
    "        criterion=criterion,\n",
    "        epoch_idx=epoch,\n",
    "    )\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    history[\"train_loss\"].append(train_loss)\n",
    "    history[\"val_loss\"].append(val_loss)\n",
    "    history[\"val_acc\"].append(val_acc)\n",
    "    history[\"epoch_throughput_imgs_per_sec\"].append(avg_throughput)\n",
    "\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{EPOCHS} summary:\")\n",
    "    print(f\"  Train loss:       {train_loss:.4f}\")\n",
    "    print(f\"  Val loss:         {val_loss:.4f}\")\n",
    "    print(f\"  Val accuracy:     {val_acc * 100:.2f}%\")\n",
    "    print(f\"  Avg throughput:   {avg_throughput:.1f} images/sec\")\n",
    "\n",
    "total_train_end = time.time()\n",
    "total_training_time_sec = total_train_end - total_train_start\n",
    "\n",
    "peak_vram_bytes = None\n",
    "peak_vram_mb = None\n",
    "peak_vram_gb = None\n",
    "\n",
    "if device.type == \"cuda\":\n",
    "    peak_vram_bytes = torch.cuda.max_memory_allocated()\n",
    "    peak_vram_mb = peak_vram_bytes / (1024 ** 2)\n",
    "    peak_vram_gb = peak_vram_bytes / (1024 ** 3)\n",
    "\n",
    "history[\"total_training_time_sec\"] = total_training_time_sec\n",
    "history[\"peak_vram_bytes\"] = peak_vram_bytes\n",
    "history[\"peak_vram_mb\"] = peak_vram_mb\n",
    "history[\"peak_vram_gb\"] = peak_vram_gb\n",
    "history[\"best_val_acc\"] = best_val_acc\n",
    "history[\"final_val_acc\"] = history[\"val_acc\"][-1] if history[\"val_acc\"] else None\n",
    "\n",
    "print(\"\\n===== Training complete =====\")\n",
    "print(f\"Total training time: {total_training_time_sec / 60:.2f} minutes\")\n",
    "\n",
    "if peak_vram_mb is not None:\n",
    "    print(f\"Peak VRAM usage:    {peak_vram_mb:.1f} MB ({peak_vram_gb:.3f} GB)\")\n",
    "\n",
    "print(f\"Best val accuracy:  {best_val_acc * 100:.2f}%\")\n",
    "print(f\"Final val accuracy: {history['final_val_acc'] * 100:.2f}%\")\n",
    "\n",
    "print(\"\\nNOTE:\")\n",
    "print(\"- Record avg GPU Util, avg VRAM, and avg power from HWiNFO at mid-training.\")\n",
    "print(\"- Later, we can compute performance per watt = best_epoch_throughput / avg_power_W.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
